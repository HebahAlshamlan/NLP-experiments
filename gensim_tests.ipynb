{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gensim_tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqEWmdLf+VmyOJD8jJzb+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HebahAlshamlan/NLP-experiments/blob/master/gensim_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUcfABwOU1Bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "346bf1d2-d85b-486f-f952-9d68d88dd64c"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "\n",
        "# How to create a dictionary from a list of sentences?\n",
        "documents = [\"السلام عليكم ورحمة الله\", \n",
        "             \"هذا نص رائع\", \n",
        "             \"الروعة تكمن في الدراسة\", \n",
        "             \"انت أبله يا أبله انت غبي\"]\n",
        "\n",
        "documents_2 = [\"وعليكم السلام يا اخي\", \n",
        "                \"انه النص الاخر من القصيدة\", \n",
        "                \"ليت الذي بيني وبينك عامر وبين وبين الاخرين سراب\", \n",
        "                \"التمكين والتعلم والشغف يكمن داخل الإنسان\", \n",
        "                \"القمر يتبع الأرض\", \n",
        "                \"الأرق مرض\"]\n",
        "\n",
        "# Tokeniziton\n",
        "texts = [[text for text in doc.split()] for doc in documents_2]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Get information about the dictionary\n",
        "print(dictionary)\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(28 unique tokens: ['اخي', 'السلام', 'وعليكم', 'يا', 'الاخر']...)\n",
            "{'اخي': 0, 'السلام': 1, 'وعليكم': 2, 'يا': 3, 'الاخر': 4, 'القصيدة': 5, 'النص': 6, 'انه': 7, 'من': 8, 'الاخرين': 9, 'الذي': 10, 'بيني': 11, 'سراب': 12, 'عامر': 13, 'ليت': 14, 'وبين': 15, 'وبينك': 16, 'الإنسان': 17, 'التمكين': 18, 'داخل': 19, 'والتعلم': 20, 'والشغف': 21, 'يكمن': 22, 'الأرض': 23, 'القمر': 24, 'يتبع': 25, 'الأرق': 26, 'مرض': 27}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgeK7AjNYEjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8d3e91a0-efc1-4807-f213-1cb8d3c47e23"
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from smart_open import smart_open\n",
        "import os\n",
        "\n",
        "# Create gensim dictionary form a single tet file\n",
        "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt', encoding='utf-8'))\n",
        "\n",
        "# Token to Id map\n",
        "dictionary.token2id\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'الشر': 0,\n",
              " 'العالم': 1,\n",
              " 'الكون': 2,\n",
              " 'الهرب': 3,\n",
              " 'بديع': 4,\n",
              " 'جدا': 5,\n",
              " 'رايع': 6,\n",
              " 'كيف': 7,\n",
              " 'لتطهير': 8,\n",
              " 'منتشر': 9,\n",
              " 'نريد': 10,\n",
              " 'نكون': 11,\n",
              " 'هذا': 12,\n",
              " 'هنا': 13,\n",
              " 'وكل': 14}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfa0Ymx6ZYcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "8fcfe157-2565-45e7-a298-bf18315cd8fe"
      },
      "source": [
        "# -*- coding: utf8 -*-\n",
        "# this part from : https://github.com/bakrianoo/aravec\n",
        "import gensim\n",
        "import re\n",
        "\n",
        "# load the model\n",
        "model = gensim.models.Word2Vec.load('www_sg_100')\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    \n",
        "    #remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "    \n",
        "    #remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "    \n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    #trim    \n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# python 3.X\n",
        "word = clean_str(u'حقير')\n",
        "\n",
        "# find and print the most similar terms to a word\n",
        "most_similar = model.wv.most_similar( word )\n",
        "for term, score in most_similar:\n",
        "\tprint(term, score)\n",
        "\t\n",
        "# get a word vector\n",
        "word_vector = model.wv[ word ]\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "مخبول 0.8166298270225525\n",
            "جبان 0.8110706806182861\n",
            "وحقير 0.7724857330322266\n",
            "شرير 0.7699036002159119\n",
            "وغبي 0.7668312191963196\n",
            "وكذاب 0.7545673251152039\n",
            "معتوه 0.752617359161377\n",
            "وجاهل 0.7525292634963989\n",
            "جاهل 0.7502806186676025\n",
            "لئيم 0.7476541996002197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}